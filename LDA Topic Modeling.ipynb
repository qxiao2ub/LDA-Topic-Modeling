{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203ea27a",
   "metadata": {},
   "source": [
    "\n",
    "# Topic Modeling with LDA (Gensim)\n",
    "\n",
    "This notebook follows the required steps:\n",
    "\n",
    "**(a) Article Selection & Topic Estimation**  \n",
    "- Uses the attached news article (`Problem1News.txt`).  \n",
    "- Manual topic estimate: **3 topics** (based on a quick read: (i) *Reagan ad vs. original speech*, (ii) *Tariff/trade-war impacts*, (iii) *US–Canada talks & reactions*).\n",
    "\n",
    "**(b) Data Preparation**  \n",
    "- Split the article by **paragraphs** (blank lines as delimiters).  \n",
    "- Preprocess: lowercase, remove stopwords, and **stemming** (Porter).  \n",
    "- Display samples of processed paragraphs.\n",
    "\n",
    "**(c) LDA Model Implementation**  \n",
    "- Train **LDA (gensim)** with `num_topics=3` on the preprocessed paragraphs.\n",
    "\n",
    "**(d) Results Presentation**  \n",
    "- Show **top 10 words** for each topic.  \n",
    "- For each topic, show the **2 most associated paragraphs** (original text), and provide a short **2–3 word label**.\n",
    "\n",
    "*Note:* The manual estimate of 3 topics is motivated by the article's structure: it moves between (1) the advert’s editing of Reagan’s 1987 speech, (2) arguments about tariffs/free trade and historical context, and (3) immediate political reactions and trade‑talk implications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ae6bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda_new\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "ARTICLE_PATH = \"Problem1News.txt\"\n",
    "\n",
    "assert os.path.exists(ARTICLE_PATH), f\"File not found: {ARTICLE_PATH}\"\n",
    "\n",
    "\n",
    "def split_into_paragraphs(text: str):\n",
    "   \n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    paras = re.split(r'\\n\\s*\\n', text.strip())\n",
    "\n",
    "    paras = [p.strip() for p in paras if p.strip()]\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3745b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 6515\n",
      "Total paragraphs: 38\n",
      "\n",
      "--- Paragraph 0 (original) ---\n",
      "What's in Reagan advert that's caused US-Canada trade talks collapse?\n",
      "3 hours ago\n",
      "\n",
      "--- Paragraph 1 (original) ---\n",
      "Share\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(ARTICLE_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "paragraphs = split_into_paragraphs(raw_text)\n",
    "print(f\"Total characters: {len(raw_text)}\")\n",
    "print(f\"Total paragraphs: {len(paragraphs)}\\n\")\n",
    "\n",
    "for i, para in enumerate(paragraphs[:2]):\n",
    "    print(f\"--- Paragraph {i} (original) ---\")\n",
    "    print(para[:800] + (\"...\" if len(para) > 800 else \"\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbc1265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed paragraphs:\n",
      "--- Paragraph 0 (processed) ---\n",
      "['reagan', 'advert', 'caus', 'canada', 'trade', 'talk', 'collaps', 'hour', 'ago']\n",
      "\n",
      "--- Paragraph 1 (processed) ---\n",
      "['share']\n",
      "\n",
      "--- Paragraph 2 (processed) ---\n",
      "['save', 'maia', 'davi', 'getti', 'imag', 'file', 'photo', 'ronald', 'reagan', 'wear', 'brown', 'suit', 'jacket', 'dark', 'red', 'tie', 'white', 'shirt', 'slick', 'brown', 'hair', 'speak', 'microphon', 'stand', 'flag', 'blue', 'curtain', 'getti', 'imag', 'radio', 'address', 'presid', 'ronald', 'reagan', 'focus', 'impact', 'tariff', 'presid', 'donald', 'trump', 'said', 'halt', 'trade', 'negoti', 'canada', 'immedi', 'advert', 'predecessor', 'ronald', 'reagan', 'say', 'tariff', 'hurt', 'american']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.Preprocessing: lowercase, remove stopwords, stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_para(para: str):\n",
    "  \n",
    "    tokens = simple_preprocess(para, deacc=True, min_len=2, max_len=30)\n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    # stemming\n",
    "    tokens = [ps.stem(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "processed_docs = [preprocess_para(p) for p in paragraphs]\n",
    "\n",
    "print(\"Sample processed paragraphs:\")\n",
    "for i in range(min(3, len(processed_docs))):\n",
    "    print(f\"--- Paragraph {i} (processed) ---\")\n",
    "    print(processed_docs[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d029906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 263\n",
      "Sample BOW for paragraph 0: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "# 3.LDA Model Implementation, prepare dictionary & corpus\n",
    "dictionary = Dictionary(processed_docs)\n",
    "# remove too-rare and too-common tokens to reduce noise\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.7)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print(f\"Vocabulary size: {len(dictionary)}\")\n",
    "print(f\"Sample BOW for paragraph 0: {corpus[0][:15] if corpus else []}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8fa1cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words per topic:\n",
      "Topic 0: ['tariff', 'trade', 'reagan', 'high', 'long', 'minut', 'address', 'advert', 'barrier', 'run']\n",
      "Topic 1: ['tariff', 'trade', 'say', 'trump', 'talk', 'canada', 'ad', 'depress', 'legisl', 'advert']\n",
      "Topic 2: ['reagan', 'line', 'advert', 'trade', 'address', 'tariff', 'free', 'origin', 'say', 'speech']\n"
     ]
    }
   ],
   "source": [
    "# 4.Train LDA with the manual estimate of 3 topics\n",
    "NUM_TOPICS = 3\n",
    "lda = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    random_state=42,\n",
    "    passes=20,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    per_word_topics=False\n",
    ")\n",
    "\n",
    "# 5.Show 10 most frequent words per topic\n",
    "TOPN = 10\n",
    "print(\"Top words per topic:\")\n",
    "topic_top_words = {}\n",
    "for k in range(NUM_TOPICS):\n",
    "    words = lda.show_topic(k, topn=TOPN)\n",
    "    topic_top_words[k] = [w for w, _ in words]\n",
    "    print(f\"Topic {k}: {[w for w, _ in words]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722fab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Topic 0 — Label: Tariff Trade\n",
      "Top words: ['tariff', 'trade', 'reagan', 'high', 'long', 'minut', 'address', 'advert', 'barrier', 'run']\n",
      "\n",
      "Most associated paragraphs:\n",
      "\n",
      "[1] Paragraph #2 (topic prob=0.998)\n",
      "Save\n",
      "Maia Davies\n",
      "Getty Images A file photo of Ronald Reagan from the 1970s. He wears a brown suit jacket, a dark red tie, a white shirt, and has slicked back brown hair. He speaks into a microphone and stands in front of a US flag and a blue curtain.Getty Images\n",
      "The radio address made by former President Ronald Reagan focused on the impact of tariffs\n",
      "US President Donald Trump has said he will halt all trade negotiations with Canada immediately over an advert in which his predecessor Ronald Reagan says tariffs \"hurt every American\".\n",
      "\n",
      "[2] Paragraph #23 (topic prob=0.997)\n",
      "\"What eventually occurs is: First, homegrown industries start relying on government protection in the form of high tariffs. They stop competing and stop making the innovative management and technological changes they need to succeed in world markets. And then, while all this is going on, something even worse occurs. High tariffs inevitably lead to retaliation by foreign countries and the triggering of fierce trade wars.\"\n",
      "================================================================================\n",
      "Topic 1 — Label: Tariff Trade\n",
      "Top words: ['tariff', 'trade', 'say', 'trump', 'talk', 'canada', 'ad', 'depress', 'legisl', 'advert']\n",
      "\n",
      "Most associated paragraphs:\n",
      "\n",
      "[1] Paragraph #36 (topic prob=0.994)\n",
      "The final chunk of his speech is omitted from the ad - in which he says he is determined \"to spare the American people the protectionist legislation that destroys prosperity\" and criticises opponents in Congress who \"want to go for the quick political advantage\" and \"forget\" the millions of jobs involved in trade.\n",
      "\n",
      "[2] Paragraph #3 (topic prob=0.990)\n",
      "The ad, sponsored by Canada's province of Ontario and released last week, features excerpts of an address Reagan gave in 1987 focusing on foreign trade.\n",
      "================================================================================\n",
      "Topic 2 — Label: Reagan Line\n",
      "Top words: ['reagan', 'line', 'advert', 'trade', 'address', 'tariff', 'free', 'origin', 'say', 'speech']\n",
      "\n",
      "Most associated paragraphs:\n",
      "\n",
      "[1] Paragraph #31 (topic prob=0.996)\n",
      "In the original, Reagan praises the economic benefits of free trade and continues: \"Now, that message of free trade is one I conveyed to Canada's leaders a few weeks ago, and it was warmly received there. Indeed, throughout the world there's a growing realisation that the way to prosperity for all nations is rejecting protectionist legislation and promoting fair and free competition.\"\n",
      "\n",
      "[2] Paragraph #10 (topic prob=0.996)\n",
      "The 1987 radio speech - Address to the Nation on Free and Fair Trade - begins with Reagan saying Japan's prime minister will visit the White House and \"recent disagreements\" on trade will be discussed. Reagan had recently placed tariffs on some Japanese goods over a trade agreement dispute.\n"
     ]
    }
   ],
   "source": [
    "# 5.For each paragraph, compute topic distribution and record top topic and score\n",
    "doc_topics = []\n",
    "for doc_idx, bow in enumerate(corpus):\n",
    "    topic_dist = lda.get_document_topics(bow, minimum_probability=0.0)\n",
    "    topic_dict = {t: float(p) for t, p in topic_dist}\n",
    "    doc_topics.append((doc_idx, topic_dict))\n",
    "\n",
    "# For each topic, pick the top-2 paragraphs by probability of that topic\n",
    "top_docs_by_topic = {}\n",
    "for k in range(NUM_TOPICS):\n",
    "    ranked = sorted(doc_topics, key=lambda x: x[1].get(k, 0.0), reverse=True)\n",
    "    top_docs_by_topic[k] = ranked[:2]\n",
    "\n",
    "# Heuristic 2–3 word labels: try to pick salient words; default to first two top words\n",
    "def propose_label(words):\n",
    "    candidates = words[:3]\n",
    "    label = \" \".join(w.capitalize() for w in candidates[:2])\n",
    "    return label\n",
    "\n",
    "topic_labels = {k: propose_label(topic_top_words[k]) for k in range(NUM_TOPICS)}\n",
    "\n",
    "# Present results\n",
    "for k in range(NUM_TOPICS):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Topic {k} — Label: {topic_labels[k]}\")\n",
    "    print(f\"Top words: {topic_top_words[k]}\")\n",
    "    print(\"\\nMost associated paragraphs:\")\n",
    "    for rank, (doc_idx, tdist) in enumerate(top_docs_by_topic[k], start=1):\n",
    "        score = tdist.get(k, 0.0)\n",
    "        print(f\"\\n[{rank}] Paragraph #{doc_idx} (topic prob={score:.3f})\")\n",
    "        para_text = paragraphs[doc_idx]\n",
    "        # Print up to 700 chars to keep it readable\n",
    "        snippet = para_text[:700] + (\"...\" if len(para_text) > 700 else \"\")\n",
    "        print(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed2a55",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Interpretation\n",
    "\n",
    "- The **labels** are heuristic and derived from each topic’s top words.  \n",
    "- Use the listed **most associated paragraphs** to refine the labels if needed.  \n",
    "- You can re-run the LDA with a different `NUM_TOPICS` to see how topics regroup.\n",
    "\n",
    "**Manual Estimate Recap:** 3 topics — *Reagan ad vs. original speech*, *Tariff/trade-war impacts*, *US–Canada talks & reactions*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
